# CLAUDE.md - LLM 审议团技术说明文档

本文档包含该项目的技术细节、架构设计决策及重要实现说明，供后续开发参考。

## 项目概述

LLM 审议团（LLM Council）是一个**三阶段审议系统**，多个大语言模型（LLM）将协同协作回答用户问题。其核心创新点在于**第二阶段的匿名同行评审机制**，可避免模型间出现偏袒行为。

## 架构设计

### 后端结构（`backend/`）

**`config.py`**

- 包含 `COUNCIL_MODELS` 配置（OpenRouter 模型标识符列表）
- 包含 `CHAIRMAN_MODEL` 配置（负责合成最终答案的「主席模型」）
- 从 `.env` 文件读取环境变量 `OPENROUTER_API_KEY`
- 后端运行端口为 **8001**（非 8000，因用户原有应用占用 8000 端口）

**`openrouter.py`**

- `query_model()`：单个模型的异步查询函数
- `query_models_parallel()`：基于 `asyncio.gather()` 的并行查询函数
- 返回格式：字典类型，包含 `content`（响应内容）和可选的 `reasoning_details`（推理细节）
- 优雅降级机制：模型调用失败时返回 `None`，不影响其他成功响应的处理

**`council.py` - 核心逻辑层**

- `stage1_collect_responses()`：向所有审议团模型发起并行查询，收集初始响应

- ```
  stage2_collect_rankings()
  ```

  ：

  - 将原始响应匿名化为「响应 A、响应 B、响应 C」等标签
  - 生成 `label_to_model` 映射表（用于后续去匿名化）
  - 提示模型对匿名响应进行评估和排序（严格要求格式规范）
  - 返回值：元组 `(rankings_list, label_to_model_dict)`（排序结果列表 + 标签 - 模型映射表）
  - 每个排序结果包含：原始评估文本 + `parsed_ranking`（解析后的排序列表）

- `stage3_synthesize_final()`：主席模型结合所有阶段 1 响应和阶段 2 排序结果，合成最终答案

- `parse_ranking_from_text()`：提取评估文本中「FINAL RANKING:」部分的内容，支持编号列表和纯文本格式

- `calculate_aggregate_rankings()`：基于所有模型的评审结果，计算响应的「平均排名位置」

**`storage.py`**

- 基于 JSON 的对话存储机制，数据目录为 `data/conversations/`
- 单条对话结构：`{id, created_at, messages[]}`（对话 ID、创建时间、消息列表）
- 助手消息结构：`{role, stage1, stage2, stage3}`（角色、三阶段响应数据）
- 注意：元数据（标签 - 模型映射表、汇总排名）**不持久化存储**，仅通过 API 实时返回

**`main.py`**

- 基于 FastAPI 构建的后端应用，已启用 CORS 跨域支持（允许 [localhost:5173](https://localhost:5173/) 和 [localhost:3000](https://localhost:3000/) 访问）
- POST 接口 `/api/conversations/{id}/message`：返回三阶段数据及元数据
- 元数据包含：`label_to_model` 映射表 + `aggregate_rankings`（汇总排名）

### 前端结构（`frontend/src/`）

**`App.jsx`**

- 核心协调组件：管理对话列表和当前活跃对话
- 处理消息发送逻辑和元数据存储
- 重要说明：元数据仅存储在前端状态中（用于展示），不持久化到后端 JSON 文件

**`components/ChatInterface.jsx`**

- 多行文本输入框（默认 3 行，支持 resize 调整大小）
- 快捷键逻辑：Enter 发送消息，Shift+Enter 换行
- 用户消息外层包裹 `markdown-content` 类（用于统一内边距样式）

**`components/Stage1.jsx`**

- 标签页视图：展示各个模型的原始响应
- 使用 ReactMarkdown 渲染内容，外层包裹 `markdown-content` 样式类

**`components/Stage2.jsx`**

- 核心功能：标签页视图展示「每个模型的原始评估文本」
- 去匿名化处理：在**前端本地**完成（模型接收的是匿名标签）
- 解析验证：每个评估文本下方显示「提取的排名结果」，方便用户验证解析准确性
- 汇总排名展示：显示每个响应的平均排名位置和获得的投票数
- 说明文本：明确标注「加粗的模型名称仅为可读性优化，原始评估使用匿名标签」

**`components/Stage3.jsx`**

- 展示主席模型合成的最终答案
- 背景色：淡绿色（`#f0fff0`），用于突出最终结论

**样式文件（`\*.css`）**

- 主题：浅色模式（无深色模式支持）
- 主色调：`#4a90e2`（蓝色）
- 全局 Markdown 样式：`index.css` 中定义 `.markdown-content` 类
- 统一内边距：所有 Markdown 内容均设置 12px 内边距，避免视觉拥挤

## 关键设计决策

### 第二阶段提示词格式

第二阶段的提示词具有严格格式要求，确保输出可解析：

plaintext

```plaintext
1. 先对每个响应进行单独评估
2. 必须包含「FINAL RANKING:」标题
3. 编号列表格式：例如「1. 响应 C」「2. 响应 A」等
4. 排名部分之后不得添加额外文本
```

该严格格式既保证了解析可靠性，又不影响模型进行深度评估。

### 去匿名化策略

- 模型接收的响应：仅显示「响应 A」「响应 B」等匿名标签
- 后端生成映射表：`{"响应 A": "openai/gpt-5.1", ...}`（匿名标签与真实模型的对应关系）
- 前端展示：真实模型名称以**加粗**形式显示（提升可读性）
- 用户知情权：明确告知用户「原始评估过程使用匿名标签」
- 核心目标：避免模型偏见，同时保证结果透明度

### 错误处理原则

- 部分模型失败时：继续使用成功的响应（优雅降级）
- 请求容错：不因单个模型失败导致整个请求崩溃
- 错误日志：后台记录错误，但不向用户暴露（除非所有模型均失败）

### UI/UX 透明度设计

- 可追溯性：所有原始输出均可通过标签页查看
- 解析验证：原始文本下方同步展示解析后的排名，方便用户校验
- 系统可解释性：用户可验证系统对模型输出的解读逻辑
- 核心价值：建立用户信任，同时便于调试边缘案例

## 重要实现细节

### 相对导入规范

所有后端模块均使用**相对导入**（例如 `from .config import ...`），而非绝对导入。这是确保 Python 模块系统正常工作的关键（运行命令需为 `python -m backend.main`）。

### 端口配置说明

- 后端：8001（从 8000 调整，避免端口冲突）
- 前端：5173（Vite 构建工具默认端口）
- 变更注意：若需修改端口，需同步更新 `backend/main.py` 和 `frontend/src/api.js` 中的配置

### Markdown 渲染要求

所有 ReactMarkdown 组件必须包裹在 `<div className="markdown-content">` 中，以确保统一的间距样式（该类在 `index.css` 中全局定义）。

### 模型配置方式

模型列表硬编码在 `backend/config.py` 中。主席模型可与审议团模型相同或不同，当前默认使用 Gemini 作为主席模型（按用户偏好设置）。

## 常见问题与注意事项

1. **模块导入错误**：必须从项目根目录执行 `python -m backend.main` 启动后端，不可从 `backend` 目录直接运行
2. **CORS 跨域问题**：前端地址必须在 `main.py` 的 CORS 中间件允许列表中
3. **排名解析失败**：若模型未遵循格式要求，系统将通过备用正则表达式提取所有「响应 X」格式的内容，并按出现顺序排序
4. **元数据缺失**：元数据为临时数据（不持久化），仅在 API 响应中提供

## 未来增强方向

- UI 配置化：支持通过前端界面配置审议团模型和主席模型（替代当前的配置文件方式）
- 流式响应：支持流式输出，替代批量加载（减少等待时间）
- 导出功能：支持将对话导出为 Markdown/PDF 格式
- 性能分析：模型长期性能跟踪与分析
- 自定义排序规则：支持用户自定义排名标准（不仅限于准确性 / 洞察力）
- 推理型模型支持：为 o1 等推理专用模型提供特殊处理逻辑

## 测试说明

可使用 `test_openrouter.py` 脚本验证 API 连通性，在将模型添加到审议团前测试不同的模型标识符。该脚本支持测试流式和非流式两种调用模式。

## 数据流总结

plaintext

```plaintext
用户查询
    ↓
阶段1：并行查询 → [各模型独立响应]
    ↓
阶段2：匿名化处理 → 并行排名查询 → [评估文本 + 解析后的排名]
    ↓
汇总排名计算 → [按平均排名排序]
    ↓
阶段3：主席模型结合全量上下文合成最终答案
    ↓
返回结果：{stage1, stage2, stage3, metadata}（三阶段数据 + 元数据）
    ↓
前端展示：标签页视图 + 解析验证界面
```